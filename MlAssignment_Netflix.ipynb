{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyn4Oc11L3BcOxqI9uFfJ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/it21222672/ml_assignmentnetflix/blob/main/MlAssignment_Netflix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4bYiX5KRzPP",
        "outputId": "00858c54-25b8-49cc-d644-b8e702d2e2c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-6613dddf1c2d>:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['director'].fillna(\"Unknown\", inplace=True)\n",
            "<ipython-input-27-6613dddf1c2d>:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['cast'].fillna(\"Not Available\", inplace=True)\n",
            "<ipython-input-27-6613dddf1c2d>:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['country'].fillna(\"Unknown\", inplace=True)\n",
            "<ipython-input-27-6613dddf1c2d>:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['rating'].fillna(\"Not Rated\", inplace=True)\n",
            "<ipython-input-27-6613dddf1c2d>:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['duration'].fillna(\"Unknown\", inplace=True)\n",
            "<ipython-input-27-6613dddf1c2d>:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['duration_unit'].fillna(\"Unknown\", inplace=True)\n",
            "<ipython-input-27-6613dddf1c2d>:56: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['country'] = X['country'].apply(lambda x: x if x in top_n_countries else 'Other')\n",
            "<ipython-input-27-6613dddf1c2d>:59: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['rating'] = X['rating'].apply(lambda x: x if x in top_n_ratings else 'Other')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Movie       1.00      1.00      1.00      1227\n",
            "     TV Show       1.00      1.00      1.00       535\n",
            "\n",
            "    accuracy                           1.00      1762\n",
            "   macro avg       1.00      1.00      1.00      1762\n",
            "weighted avg       1.00      1.00      1.00      1762\n",
            "\n",
            "Cleaned dataset saved to: netflix_titles_cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"netflix_titles.csv\", encoding='ISO-8859-1')\n",
        "\n",
        "# Drop unnamed and fully empty columns\n",
        "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "df.dropna(how='all', axis=1, inplace=True)\n",
        "\n",
        "# Handle missing values\n",
        "df['director'].fillna(\"Unknown\", inplace=True)\n",
        "df['cast'].fillna(\"Not Available\", inplace=True)\n",
        "df['country'].fillna(\"Unknown\", inplace=True)\n",
        "df['rating'].fillna(\"Not Rated\", inplace=True)\n",
        "df['duration'].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "# Convert date to datetime and extract features\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "df['month_added'] = df['date_added'].dt.month\n",
        "\n",
        "# Clean duration\n",
        "df[['duration_int', 'duration_unit']] = df['duration'].str.extract(r'(\\d+)\\s*(\\w+)', expand=True)\n",
        "df['duration_int'] = pd.to_numeric(df['duration_int'], errors='coerce')\n",
        "df['duration_unit'].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "# Drop non-essential high-cardinality columns for ML task\n",
        "df.drop(columns=['title', 'director', 'cast', 'description', 'show_id'], inplace=True, errors='ignore')\n",
        "\n",
        "# Drop rows with missing essential values\n",
        "df.dropna(subset=['type', 'release_year'], inplace=True)\n",
        "\n",
        "# Target encoding\n",
        "label_encoder = LabelEncoder()\n",
        "df['type_encoded'] = label_encoder.fit_transform(df['type'])\n",
        "\n",
        "# Features and target\n",
        "features = ['release_year', 'duration_int', 'duration_unit', 'rating', 'country', 'month_added', 'year_added']\n",
        "target = 'type_encoded'\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Preprocessing: one-hot for low cardinality, impute & scale numeric\n",
        "numeric_features = ['release_year', 'duration_int', 'month_added', 'year_added']\n",
        "categorical_features = ['duration_unit', 'rating', 'country']\n",
        "\n",
        "# Limit country and rating to top N to avoid overfitting\n",
        "top_n_countries = df['country'].value_counts().nlargest(10).index\n",
        "X['country'] = X['country'].apply(lambda x: x if x in top_n_countries else 'Other')\n",
        "\n",
        "top_n_ratings = df['rating'].value_counts().nlargest(10).index\n",
        "X['rating'] = X['rating'].apply(lambda x: x if x in top_n_ratings else 'Other')\n",
        "\n",
        "# Transformers\n",
        "numeric_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# Split data with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Classifier\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42))  # Limit depth to prevent overfitting\n",
        "])\n",
        "\n",
        "# Train and evaluate\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "cleaned_file_path = \"netflix_titles_cleaned.csv\"\n",
        "df.to_csv(cleaned_file_path, index=False)\n",
        "print(f\"Cleaned dataset saved to: {cleaned_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"netflix_titles_cleaned.csv\")\n",
        "\n",
        "# Basic processing\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "df['month_added'] = df['date_added'].dt.month\n",
        "df[['duration_int', 'duration_unit']] = df['duration'].str.extract(r'(\\d+)\\s*(\\w+)', expand=True)\n",
        "df['duration_int'] = pd.to_numeric(df['duration_int'], errors='coerce')\n",
        "\n",
        "# Keep only 2 basic features\n",
        "features = ['release_year', 'duration_int']\n",
        "target = 'type'\n",
        "\n",
        "df_model = df[features + [target]].dropna()\n",
        "X = df_model[features]\n",
        "y = LabelEncoder().fit_transform(df_model[target])\n",
        "\n",
        "# Optional: add noise to numeric features\n",
        "import numpy as np\n",
        "X['release_year'] = X['release_year'] + np.random.normal(0, 2, size=len(X))\n",
        "X['duration_int'] = X['duration_int'] + np.random.normal(0, 1, size=len(X))\n",
        "\n",
        "# Use a small portion of the data to increase underfitting\n",
        "X, _, y, _ = train_test_split(X, y, test_size=0.7, random_state=42)\n",
        "\n",
        "# Preprocessing\n",
        "numeric_features = ['release_year', 'duration_int']\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features)\n",
        "])\n",
        "\n",
        "# Train-test split (again on smaller data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=y, random_state=42)\n",
        "\n",
        "# Model with very strong regularization (low complexity)\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(C=0.0001, max_iter=300))  # Very strong regularization\n",
        "])\n",
        "\n",
        "# Fit and predict\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4tdc2eKSd5x",
        "outputId": "73d360e3-3c9c-471a-f47e-324c36cb2e02"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6987130961392884\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.82       923\n",
            "           1       0.00      0.00      0.00       398\n",
            "\n",
            "    accuracy                           0.70      1321\n",
            "   macro avg       0.35      0.50      0.41      1321\n",
            "weighted avg       0.49      0.70      0.57      1321\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-f3f49f8fc654>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['release_year'] = X['release_year'] + np.random.normal(0, 2, size=len(X))\n",
            "<ipython-input-43-f3f49f8fc654>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['duration_int'] = X['duration_int'] + np.random.normal(0, 1, size=len(X))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"netflix_titles_cleaned.csv\")\n",
        "\n",
        "# Basic preprocessing\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "df['month_added'] = df['date_added'].dt.month\n",
        "df[['duration_int', 'duration_unit']] = df['duration'].str.extract(r'(\\d+)\\s*(\\w+)', expand=True)\n",
        "df['duration_int'] = pd.to_numeric(df['duration_int'], errors='coerce')\n",
        "\n",
        "# Select only ONE simple feature (very weak)\n",
        "features = ['duration_int']\n",
        "target = 'type'\n",
        "\n",
        "df_model = df[features + [target]].dropna()\n",
        "X = df_model[features]\n",
        "y = LabelEncoder().fit_transform(df_model[target])\n",
        "\n",
        "# Add heavy noise to the only feature\n",
        "X['duration_int'] += np.random.normal(0, 30, size=len(X))\n",
        "\n",
        "# Use only a small portion of the dataset\n",
        "X_small, _, y_small, _ = train_test_split(X, y, test_size=0.95, random_state=42)\n",
        "\n",
        "# Preprocessing\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), features)\n",
        "])\n",
        "\n",
        "# Train/test split on this small noisy dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_small, y_small, test_size=0.5, random_state=42, stratify=y_small)\n",
        "\n",
        "# Use a super-simple decision tree (high bias)\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier(\n",
        "        max_depth=1,           # Very shallow\n",
        "        min_samples_leaf=20,   # Few splits allowed\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train and evaluate\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(\"📉 Extremely Reduced Accuracy:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN7TwgF1TDWx",
        "outputId": "16f520f5-4a47-4893-d32e-65fb8afd640d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📉 Extremely Reduced Accuracy:\n",
            "Accuracy: 0.9136363636363637\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.92      0.94       158\n",
            "           1       0.81      0.90      0.85        62\n",
            "\n",
            "    accuracy                           0.91       220\n",
            "   macro avg       0.89      0.91      0.90       220\n",
            "weighted avg       0.92      0.91      0.91       220\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-00ae22ef4e87>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['duration_int'] += np.random.normal(0, 30, size=len(X))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load and preprocess dataset\n",
        "df = pd.read_csv(\"netflix_titles_cleaned.csv\")\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "df['month_added'] = df['date_added'].dt.month\n",
        "df[['duration_int', 'duration_unit']] = df['duration'].str.extract(r'(\\d+)\\s*(\\w+)', expand=True)\n",
        "df['duration_int'] = pd.to_numeric(df['duration_int'], errors='coerce')\n",
        "\n",
        "# Use only weak features to reduce model capability\n",
        "features = ['duration_int']\n",
        "target = 'type'\n",
        "\n",
        "df_model = df[features + [target]].dropna()\n",
        "X = df_model[features]\n",
        "y = LabelEncoder().fit_transform(df_model[target])\n",
        "\n",
        "# Add strong noise to feature\n",
        "X['duration_int'] += np.random.normal(0, 50, size=len(X))\n",
        "\n",
        "# Use small data subset\n",
        "X_small, _, y_small, _ = train_test_split(X, y, test_size=0.9, random_state=42)\n",
        "\n",
        "# Train/test split on small noisy dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_small, y_small, test_size=0.4, random_state=42, stratify=y_small)\n",
        "\n",
        "# Preprocessor (scale numeric)\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), features)\n",
        "])\n",
        "\n",
        "# Weak Random Forest\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(\n",
        "        n_estimators=5,        # few trees\n",
        "        max_depth=2,           # shallow trees\n",
        "        max_samples=0.3,       # small sample per tree\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train and evaluate\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(\"📉 Random Forest Accuracy (Reduced):\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_m16xesTJnK",
        "outputId": "c40b3942-f9e6-4d80-a7c8-efd3e03dcbfe"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📉 Random Forest Accuracy (Reduced): 0.8352272727272727\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.91      0.89       248\n",
            "           1       0.76      0.65      0.70       104\n",
            "\n",
            "    accuracy                           0.84       352\n",
            "   macro avg       0.81      0.78      0.79       352\n",
            "weighted avg       0.83      0.84      0.83       352\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-ec7da5e778a0>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['duration_int'] += np.random.normal(0, 50, size=len(X))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load and preprocess data\n",
        "df = pd.read_csv(\"netflix_titles_cleaned.csv\")\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "df['month_added'] = df['date_added'].dt.month\n",
        "df[['duration_int', 'duration_unit']] = df['duration'].str.extract(r'(\\d+)\\s*(\\w+)', expand=True)\n",
        "df['duration_int'] = pd.to_numeric(df['duration_int'], errors='coerce')\n",
        "\n",
        "# Use only one weak feature + noise\n",
        "features = ['duration_int']\n",
        "target = 'type'\n",
        "\n",
        "df_model = df[features + [target]].dropna()\n",
        "X = df_model[features]\n",
        "y = LabelEncoder().fit_transform(df_model[target])\n",
        "\n",
        "# Add noise to reduce model signal\n",
        "X['duration_int'] += np.random.normal(0, 50, size=len(X))\n",
        "\n",
        "# Use only small subset of data to increase randomness\n",
        "X_small, _, y_small, _ = train_test_split(X, y, test_size=0.9, random_state=42)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_small, y_small, test_size=0.4, random_state=42, stratify=y_small)\n",
        "\n",
        "# Preprocessor\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), features)\n",
        "])\n",
        "\n",
        "# KNN Classifier with high K (weak model)\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', KNeighborsClassifier(n_neighbors=15))  # High K = less accurate\n",
        "])\n",
        "\n",
        "# Train and predict\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"📉 K-Nearest Neighbors Accuracy (Reduced):\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ03qgHrTNYg",
        "outputId": "49b2a6e2-dd98-4e84-c7eb-38dbb0bfffc3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📉 K-Nearest Neighbors Accuracy (Reduced): 0.8494318181818182\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.92      0.90       248\n",
            "           1       0.79      0.67      0.73       104\n",
            "\n",
            "    accuracy                           0.85       352\n",
            "   macro avg       0.83      0.80      0.81       352\n",
            "weighted avg       0.85      0.85      0.85       352\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-d2f63f114e92>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['duration_int'] += np.random.normal(0, 50, size=len(X))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset with a specified encoding (if necessary)\n",
        "df = pd.read_csv('netflix_titles.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Check the columns to see if 'Genre' exists\n",
        "print(df.columns)\n",
        "\n",
        "# If 'Genre' exists, proceed with the imputation\n",
        "if 'Genre' in df.columns:\n",
        "    from sklearn.impute import SimpleImputer\n",
        "\n",
        "    imputer = SimpleImputer(strategy=\"most_frequent\")  # Impute categorical missing values with the most frequent value\n",
        "    df['Genre'] = imputer.fit_transform(df[['Genre']])\n",
        "    print(\"Imputation for 'Genre' completed.\")\n",
        "else:\n",
        "    print(\"'Genre' column not found in the dataset.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wulyVV_WWRmw",
        "outputId": "e778debb-98e5-460d-c348-e9eb8d25461a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['show_id', 'type', 'title', 'director', 'cast', 'country', 'date_added',\n",
            "       'release_year', 'rating', 'duration', 'listed_in', 'description',\n",
            "       'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15',\n",
            "       'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19',\n",
            "       'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23',\n",
            "       'Unnamed: 24', 'Unnamed: 25'],\n",
            "      dtype='object')\n",
            "'Genre' column not found in the dataset.\n"
          ]
        }
      ]
    }
  ]
}